# Training configuration
training:
  batch_size: 16  
  num_epochs: 50
  lr: 1.0e-4
  temperature: 0.07
  max_grad_norm: 1.0

model:
  LLM_MODEL_NAME: "emilyalsentzer/Bio_ClinicalBERT"
  MAX_REPORT_LENGTH: 512
  num_queries: 32
  hidden_dim: 768
  num_layers: 6
  num_heads: 12
  intermediate_size: 3072
  dropout: 0.1

data:
  data_root: "/root/.cache/kagglehub/datasets/hajarhamdouch/mimic-dataset-30k-xray-reports/versions/2/mimicDatatotal (1)"  
  num_workers: 4
  max_samples: null  